# VLP PTM
## Survey
2203：[Vision-Language Intelligence: Tasks, Representation Learning, and Large Models](https://arxiv.org/abs/2203.01922.pdf) by Feng Li, Hao Zhang, Yi-Fan Zhang, Shilong Liu, Jian Guo, Lionel M. Ni, PengChuan Zhang, Lei Zhang  
2202：[A Survey of Vision-Language Pre-Trained Models](https://arxiv.org/abs/2202.10936.pdf) by Yifan Du, Zikang Liu, Junyi Li, Wayne Xin Zhao  
2202[VLP: A Survey on Vision-Language Pre-training](https://arxiv.org/abs/2202.09061.pdf) by Feilong Chen, Duzhen Zhang, Minglun Han, Xiuyi Chen, Jing Shi, Shuang Xu, Bo Xu  
2109：[Survey: Transformer based Video-Language Pre-training](https://arxiv.org/abs/2109.09920.pdf) by Ludan Ruan, Qin Jin  
2002：[UniVL](https://github.com/microsoft/UniVL) from (MSR) [UniVL_ A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation](https://arxiv.org/abs/2002.06353)  

## Image-Text
### Cross-Stream
* [ViLBERT][Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks]  
* LXMERT[LXMERT: Learning Cross-Modality Encoder Representations from Transformers]  
* ERNIE-ViL[Ernie-vil: Knowledge enhanced vision-language representations through scene graph]  
### Single-Stream
* VL-BERT[Vl-bert: Pre-training of generic visual-linguistic representations]  
* Image-BERT[ Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data]  
## Video-Text
### Cross-Stream
* UniVL[Univilm: A unified video and language pre-training model for multimodal understanding and generation]  
* ActBERT[Actbert: Learning global-local video-text representations]  
### Single-Stream
* VideoBERT[Videobert: A joint model for video and language representation learning]  
* HERO[Hero: Hierarchical encoder for video+ language omni-representation pre-training]  



















