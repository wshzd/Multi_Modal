# VLP PTM
## Survey
2203：[Vision-Language Intelligence: Tasks, Representation Learning, and Large Models](https://arxiv.org/abs/2203.01922.pdf) by Feng Li, Hao Zhang, Yi-Fan Zhang, Shilong Liu, Jian Guo, Lionel M. Ni, PengChuan Zhang, Lei Zhang  
2202：[A Survey of Vision-Language Pre-Trained Models](https://arxiv.org/abs/2202.10936.pdf) by Yifan Du, Zikang Liu, Junyi Li, Wayne Xin Zhao  
2202[VLP: A Survey on Vision-Language Pre-training](https://arxiv.org/abs/2202.09061.pdf) by Feilong Chen, Duzhen Zhang, Minglun Han, Xiuyi Chen, Jing Shi, Shuang Xu, Bo Xu  
2109：[Survey: Transformer based Video-Language Pre-training](https://arxiv.org/abs/2109.09920.pdf) by Ludan Ruan, Qin Jin  
2002：[UniVL](https://github.com/microsoft/UniVL) from (MSR) [UniVL_ A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation](https://arxiv.org/abs/2002.06353)  

## Image-Text
### Cross-Stream
2019：[ViLBERT](https://github.com/facebookresearch/vilbert-multi-task )[Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks](https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf) by Lu J, Batra D, Parikh D, et al  
1908：[LXMERT](https://github.com/airsplay/lxmert)[LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/pdf/1908.07490) by Hao Tan, Mohit Bansal    
2006：[ERNIE-ViL]()[Ernie-vil: Knowledge enhanced vision-language representations through scene graph](https://arxiv.org/abs/2006.16934) by Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang    
### Single-Stream
1908：[VL-BERT](https://github.com/jackroos/VL-BERT)[Vl-bert: Pre-training of generic visual-linguistic representations](https://arxiv.org/abs/1908.08530v3) by Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai    
2001：[Image-BERT][Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data](https://arxiv.org/abs/2001.07966v1) by Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, Arun Sachet    
## Video-Text
### Cross-Stream
2002：[UniVL][Univilm: A unified video and language pre-training model for multimodal understanding and generation](https://arxiv.org/abs/2002.06353) by Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, Ming Zhou    
2020：[ActBERT][Actbert: Learning global-local video-text representations](http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_ActBERT_Learning_Global-Local_Video-Text_Representations_CVPR_2020_paper.pdf)    
### Single-Stream
1904：[VideoBERT][Videobert: A joint model for video and language representation learning](https://arxiv.org/abs/1904.01766v2) by Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid    
2005：[HERO](https://github.com/linjieli222/HERO)[Hero: Hierarchical encoder for video+ language omni-representation pre-training](https://arxiv.org/abs/2005.00200) by Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, Jingjing Liu    



















