# VLP PTM
## Survey
2203：[Vision-Language Intelligence: Tasks, Representation Learning, and Large Models](https://arxiv.org/abs/2203.01922.pdf) by Feng Li, Hao Zhang, Yi-Fan Zhang, Shilong Liu, Jian Guo, Lionel M. Ni, PengChuan Zhang, Lei Zhang  
2202：[A Survey of Vision-Language Pre-Trained Models](https://arxiv.org/abs/2202.10936.pdf) by Yifan Du, Zikang Liu, Junyi Li, Wayne Xin Zhao  
2202[VLP: A Survey on Vision-Language Pre-training](https://arxiv.org/abs/2202.09061.pdf) by Feilong Chen, Duzhen Zhang, Minglun Han, Xiuyi Chen, Jing Shi, Shuang Xu, Bo Xu  
2109：[Survey: Transformer based Video-Language Pre-training](https://arxiv.org/abs/2109.09920.pdf) by Ludan Ruan, Qin Jin  

## Image-Text
### Cross-Stream
2019：[ViLBERT](https://github.com/facebookresearch/vilbert-multi-task ) from paper [Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks](https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf) by Lu J, Batra D, Parikh D, et al  
1908：[LXMERT](https://github.com/airsplay/lxmert) from paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/pdf/1908.07490) by Hao Tan, Mohit Bansal    
2006：[ERNIE-ViL]() from paper [Ernie-vil: Knowledge enhanced vision-language representations through scene graph](https://arxiv.org/abs/2006.16934) by Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang    
### Single-Stream
1908：[VL-BERT](https://github.com/jackroos/VL-BERT) from paper [Vl-bert: Pre-training of generic visual-linguistic representations](https://arxiv.org/abs/1908.08530v3) by Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai    
2001：[Image-BERT] from paper [Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data](https://arxiv.org/abs/2001.07966v1) by Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, Arun Sachet   
### CLIP extend models
2109：[ActionCLIP](https://github.com/sallymmx/ActionCLIP) from papers [ActionCLIP: A New Paradigm for Video Action Recognition](https://arxiv.org/abs/2109.08472) by Mengmeng Wang, Jiazheng Xing, Yong Liu  
2111：[CLIP2TV]() from papers [CLIP2TV: An Empirical Study on Transformer-based Methods for Video-Text Retrieval](https://arxiv.org/abs/2111.05610) by Zijian Gao, Jingyu Liu, Sheng Chen, Dedan Chang, Hao Zhang, Jinwei Yuan  
2110：[A CLIP-Enhanced Method for Video-Language Understanding](https://arxiv.org/abs/2110.07137) by Guohao Li, Feng He, Zhifan Feng
2110：[CLIP4Caption: CLIP for Video Caption](https://arxiv.org/abs/2110.06615) by Mingkang Tang, Zhanyu Wang, Zhenhua Liu, Fengyun Rao, Dian Li, Xiu Li  
## Video-Text
### Cross-Stream
2002：[UniVL](https://github.com/microsoft/UniVL) from paper [Univilm: A unified video and language pre-training model for multimodal understanding and generation](https://arxiv.org/abs/2002.06353) by Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, Ming Zhou    
2020：[ActBERT] from paper [Actbert: Learning global-local video-text representations](http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_ActBERT_Learning_Global-Local_Video-Text_Representations_CVPR_2020_paper.pdf)    
### Single-Stream
1904：[VideoBERT] from paper [Videobert: A joint model for video and language representation learning](https://arxiv.org/abs/1904.01766v2) by Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid    
2005：[HERO](https://github.com/linjieli222/HERO) from paper [Hero: Hierarchical encoder for video+ language omni-representation pre-training](https://arxiv.org/abs/2005.00200) by Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, Jingjing Liu    

[CMU《多模态机器学习》2022课程](https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2022/schedule/)  

















